import streamlit as st
import pandas as pd
import numpy as np
from io import BytesIO
from datetime import datetime
import gc
import time

def main():
    st.title("DoÄŸalgaz TÃ¼ketim Sapma Analizi - BÃ¼yÃ¼k Veri Optimizasyonlu")
    st.markdown("2023-2024 ortalamasÄ±ndan %30 fazla sapma gÃ¶steren tesisatlarÄ± tespit edin")
    
    # Memory usage gÃ¶sterimi
    col1, col2 = st.columns([3, 1])
    with col2:
        if st.button("ğŸ”„ Bellek Temizle"):
            gc.collect()
            st.success("Bellek temizlendi!")
    
    # Sidebar iÃ§in dosya yÃ¼kleme alanlarÄ±
    st.sidebar.header("Excel DosyalarÄ±nÄ± YÃ¼kleyin")
    
    # Chunk size ayarÄ±
    chunk_size = st.sidebar.selectbox(
        "Ä°ÅŸlem ParÃ§a Boyutu:",
        [10000, 25000, 50000, 100000],
        index=1,
        help="BÃ¼yÃ¼k dosyalar iÃ§in kÃ¼Ã§Ã¼k deÄŸer seÃ§in"
    )
    
    # 2023 dosyasÄ± yÃ¼kleme
    file_2023 = st.sidebar.file_uploader(
        "2023 Veriler", 
        type=['xlsx', 'xls'],
        key="file_2023",
        help="TN, TÃ¼ketim MiktarÄ±, Tarih, SÃ¶zleÅŸme NumarasÄ± sÃ¼tunlarÄ± olmalÄ±"
    )
    
    # 2024 dosyasÄ± yÃ¼kleme
    file_2024 = st.sidebar.file_uploader(
        "2024 Veriler", 
        type=['xlsx', 'xls'],
        key="file_2024",
        help="TN, TÃ¼ketim MiktarÄ±, Tarih, SÃ¶zleÅŸme NumarasÄ± sÃ¼tunlarÄ± olmalÄ±"
    )
    
    # 2025 dosyasÄ± yÃ¼kleme
    file_2025 = st.sidebar.file_uploader(
        "2025 GÃ¼ncel Veriler", 
        type=['xlsx', 'xls'],
        key="file_2025",
        help="TN, TÃ¼ketim MiktarÄ±, Tarih, SÃ¶zleÅŸme NumarasÄ± sÃ¼tunlarÄ± olmalÄ±"
    )
    
    # EÅŸik deÄŸeri ayarÄ±
    threshold = st.sidebar.slider(
        "Sapma EÅŸiÄŸi (%)", 
        min_value=10, 
        max_value=100, 
        value=30,
        help="Bu yÃ¼zdeden fazla artÄ±ÅŸ gÃ¶steren tesisatlar raporlanacak"
    )
    
    # Sadece belirli ay aralÄ±ÄŸÄ±nÄ± analiz etme seÃ§eneÄŸi
    analyze_specific_months = st.sidebar.checkbox("Sadece belirli aylarÄ± analiz et", value=False)
    
    if analyze_specific_months:
        selected_months = st.sidebar.multiselect(
            "Analiz edilecek aylar:",
            range(1, 13),
            default=[1, 2, 3],
            format_func=lambda x: datetime(2023, x, 1).strftime("%B")
        )
    else:
        selected_months = list(range(1, 13))
    
    if file_2023 is not None and file_2024 is not None and file_2025 is not None:
        try:
            # Dosya boyutlarÄ±nÄ± gÃ¶ster
            st.info("ğŸ“ Dosya boyutlarÄ± kontrol ediliyor...")
            
            # Dosya metadata'sÄ±nÄ± al
            file_info = get_file_info(file_2023, file_2024, file_2025)
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("2023 Dosya Boyutu", f"{file_info['2023']:.1f} MB")
            with col2:
                st.metric("2024 Dosya Boyutu", f"{file_info['2024']:.1f} MB")
            with col3:
                st.metric("2025 Dosya Boyutu", f"{file_info['2025']:.1f} MB")
            
            # Ä°lk satÄ±rlarÄ± okuyarak sÃ¼tun isimlerini al
            st.info("ğŸ” SÃ¼tun yapÄ±larÄ± analiz ediliyor...")
            sample_2023 = pd.read_excel(file_2023, nrows=5)
            
            # SÃ¼tun eÅŸleÅŸtirmesi
            st.header("ğŸ”§ SÃ¼tun EÅŸleÅŸtirmesi")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                tn_col = st.selectbox(
                    "TN SÃ¼tunu:",
                    options=sample_2023.columns.tolist()
                )
                
            with col2:
                tuketim_col = st.selectbox(
                    "TÃ¼ketim SÃ¼tunu:",
                    options=sample_2023.columns.tolist()
                )
                
            with col3:
                tarih_col = st.selectbox(
                    "Tarih SÃ¼tunu:",
                    options=sample_2023.columns.tolist()
                )
                
            with col4:
                sozlesme_col = st.selectbox(
                    "SÃ¶zleÅŸme No SÃ¼tunu:",
                    options=sample_2023.columns.tolist()
                )
            
            # Ã–rnek veri gÃ¶ster
            st.subheader("ğŸ“‹ Veri Ã–nizleme (Ä°lk 5 satÄ±r)")
            display_cols = [tn_col, tuketim_col, tarih_col, sozlesme_col]
            st.dataframe(sample_2023[display_cols], use_container_width=True)
            
            if st.button("ğŸš€ BÃ¼yÃ¼k Veri Analizi BaÅŸlat", type="primary"):
                
                # Progress bar
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                try:
                    with st.spinner("BÃ¼yÃ¼k veri seti iÅŸleniyor... Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir."):
                        
                        # 1. AdÄ±m: 2023-2024 verilerini parÃ§alÄ± okuma ile iÅŸle
                        status_text.text("2023-2024 geÃ§miÅŸ veriler iÅŸleniyor...")
                        progress_bar.progress(10)
                        
                        historical_avg = process_large_historical_data(
                            file_2023, file_2024, tn_col, tuketim_col, 
                            tarih_col, sozlesme_col, chunk_size, selected_months
                        )
                        
                        if historical_avg is None or historical_avg.empty:
                            st.error("âŒ GeÃ§miÅŸ veri iÅŸlenemedi!")
                            return
                        
                        progress_bar.progress(40)
                        
                        # 2. AdÄ±m: 2025 verilerini iÅŸle
                        status_text.text("2025 gÃ¼ncel veriler iÅŸleniyor...")
                        current_data = process_large_current_data(
                            file_2025, tn_col, tuketim_col, tarih_col, 
                            sozlesme_col, chunk_size, selected_months
                        )
                        
                        if current_data is None or current_data.empty:
                            st.error("âŒ 2025 verisi iÅŸlenemedi!")
                            return
                            
                        progress_bar.progress(70)
                        
                        # 3. AdÄ±m: Sapma analizini yap
                        status_text.text("Sapma analizi yapÄ±lÄ±yor...")
                        deviation_results = analyze_large_deviations(
                            historical_avg, current_data, threshold
                        )
                        
                        progress_bar.progress(90)
                        
                        # 4. AdÄ±m: SonuÃ§larÄ± gÃ¶ster
                        status_text.text("SonuÃ§lar hazÄ±rlanÄ±yor...")
                        display_large_results(deviation_results, threshold)
                        
                        progress_bar.progress(100)
                        status_text.text("âœ… Analiz tamamlandÄ±!")
                        
                        # BelleÄŸi temizle
                        del historical_avg, current_data
                        gc.collect()
                        
                except Exception as e:
                    st.error(f"âŒ BÃ¼yÃ¼k veri iÅŸleme hatasÄ±: {str(e)}")
                    st.info("ğŸ’¡ Chunk boyutunu kÃ¼Ã§Ã¼ltmeyi deneyin veya belleÄŸi temizleyin")
                        
        except Exception as e:
            st.error(f"âŒ Dosya okuma hatasÄ±: {str(e)}")
            st.info("ğŸ’¡ DosyalarÄ±n Excel formatÄ±nda ve eriÅŸilebilir olduÄŸundan emin olun")
    
    else:
        st.info("ğŸ“‚ Analiz yapmak iÃ§in 2023, 2024 ve 2025 Excel dosyalarÄ±nÄ± yÃ¼kleyin.")
        
        # BÃ¼yÃ¼k veri iÃ§in Ã¶neriler
        st.header("ğŸ’¡ BÃ¼yÃ¼k Veri Seti Ä°Ã§in Ã–neriler")
        
        recommendations = """
        **Performans Ä°puÃ§larÄ±:**
        - Chunk boyutunu 25,000-50,000 arasÄ±nda tutun
        - Sadece gerekli aylarÄ± analiz edin 
        - Ä°ÅŸlem sÄ±rasÄ±nda diÄŸer uygulamalarÄ± kapatÄ±n
        - En az 8GB RAM Ã¶nerilir
        - SonuÃ§larÄ± CSV olarak indirip Excel'de aÃ§Ä±n
        """
        st.markdown(recommendations)
        
        # Ã–rnek veri formatÄ±
        st.header("ğŸ“‹ Beklenen Excel FormatÄ±")
        example_data = pd.DataFrame({
            'TN': ['TN001', 'TN002', 'TN001', 'TN002'],
            'TÃ¼ketim MiktarÄ±': [1250.50, 890.25, 1180.30, 920.15],
            'Tarih': ['2023-01-01', '2023-01-01', '2023-02-01', '2023-02-01'],
            'SÃ¶zleÅŸme NumarasÄ±': ['SZ123', 'SZ124', 'SZ123', 'SZ124']
        })
        st.dataframe(example_data, use_container_width=True)

def get_file_info(file1, file2, file3):
    """Dosya boyutlarÄ±nÄ± MB cinsinden dÃ¶ndÃ¼r"""
    try:
        sizes = {}
        for name, file in [('2023', file1), ('2024', file2), ('2025', file3)]:
            file.seek(0, 2)  # Dosya sonuna git
            size = file.tell()  # Boyutu al
            file.seek(0)  # BaÅŸa dÃ¶n
            sizes[name] = size / (1024 * 1024)  # MB'ye Ã§evir
        return sizes
    except:
        return {'2023': 0, '2024': 0, '2025': 0}

def process_large_historical_data(file_2023, file_2024, tn_col, tuketim_col, 
                                  tarih_col, sozlesme_col, chunk_size, selected_months):
    """BÃ¼yÃ¼k geÃ§miÅŸ veriyi parÃ§alÄ± iÅŸle"""
    try:
        historical_data = []
        
        # 2023 dosyasÄ±nÄ± parÃ§alÄ± oku
        st.info("ğŸ“Š 2023 dosyasÄ± parÃ§alÄ± okunuyor...")
        for i, chunk in enumerate(pd.read_excel(file_2023, chunksize=chunk_size)):
            st.text(f"2023 - ParÃ§a {i+1} iÅŸleniyor ({len(chunk)} satÄ±r)")
            
            processed_chunk = process_chunk(
                chunk, tn_col, tuketim_col, tarih_col, sozlesme_col, 
                2023, selected_months
            )
            
            if processed_chunk is not None and not processed_chunk.empty:
                historical_data.append(processed_chunk)
        
        # 2024 dosyasÄ±nÄ± parÃ§alÄ± oku
        st.info("ğŸ“Š 2024 dosyasÄ± parÃ§alÄ± okunuyor...")
        for i, chunk in enumerate(pd.read_excel(file_2024, chunksize=chunk_size)):
            st.text(f"2024 - ParÃ§a {i+1} iÅŸleniyor ({len(chunk)} satÄ±r)")
            
            processed_chunk = process_chunk(
                chunk, tn_col, tuketim_col, tarih_col, sozlesme_col, 
                2024, selected_months
            )
            
            if processed_chunk is not None and not processed_chunk.empty:
                historical_data.append(processed_chunk)
        
        if not historical_data:
            st.error("âŒ Ä°ÅŸlenebilir geÃ§miÅŸ veri bulunamadÄ±!")
            return None
        
        # TÃ¼m parÃ§alarÄ± birleÅŸtir
        st.info("ğŸ”— Veriler birleÅŸtiriliyor...")
        combined_df = pd.concat(historical_data, ignore_index=True)
        
        # Bellek optimizasyonu
        del historical_data
        gc.collect()
        
        # Ortalama hesapla (optimized groupby)
        st.info("ğŸ“ˆ TN bazÄ±nda ortalamalar hesaplanÄ±yor...")
        avg_data = combined_df.groupby(['TN', 'Sozlesme_No'], as_index=False).agg({
            'Tuketim': ['mean', 'count']
        })
        
        # Column flatten
        avg_data.columns = ['TN', 'Sozlesme_No', 'Ortalama_Tuketim', 'Kayit_Sayisi']
        
        # En az 2 kayÄ±t olanlarÄ± al (gÃ¼venilirlik iÃ§in)
        avg_data = avg_data[avg_data['Kayit_Sayisi'] >= 2]
        
        st.success(f"âœ… {len(avg_data)} tesisat iÃ§in geÃ§miÅŸ ortalama hesaplandÄ±")
        
        return avg_data[['TN', 'Sozlesme_No', 'Ortalama_Tuketim']]
        
    except Exception as e:
        st.error(f"GeÃ§miÅŸ veri iÅŸleme hatasÄ±: {str(e)}")
        return None

def process_large_current_data(file_2025, tn_col, tuketim_col, tarih_col, 
                               sozlesme_col, chunk_size, selected_months):
    """BÃ¼yÃ¼k 2025 verisini parÃ§alÄ± iÅŸle"""
    try:
        current_data = []
        
        st.info("ğŸ“Š 2025 dosyasÄ± parÃ§alÄ± okunuyor...")
        for i, chunk in enumerate(pd.read_excel(file_2025, chunksize=chunk_size)):
            st.text(f"2025 - ParÃ§a {i+1} iÅŸleniyor ({len(chunk)} satÄ±r)")
            
            processed_chunk = process_chunk(
                chunk, tn_col, tuketim_col, tarih_col, sozlesme_col, 
                2025, selected_months
            )
            
            if processed_chunk is not None and not processed_chunk.empty:
                # Ay bilgisi ekle
                processed_chunk['Ay'] = processed_chunk['Tarih'].dt.month
                processed_chunk['Ay_Adi'] = processed_chunk['Tarih'].dt.strftime('%Y-%m')
                current_data.append(processed_chunk)
        
        if not current_data:
            st.error("âŒ Ä°ÅŸlenebilir 2025 verisi bulunamadÄ±!")
            return None
        
        # BirleÅŸtir
        combined_current = pd.concat(current_data, ignore_index=True)
        
        # Bellek temizle
        del current_data
        gc.collect()
        
        st.success(f"âœ… 2025 verisi hazÄ±rlandÄ±: {len(combined_current)} kayÄ±t")
        
        return combined_current
        
    except Exception as e:
        st.error(f"2025 veri iÅŸleme hatasÄ±: {str(e)}")
        return None

def process_chunk(chunk, tn_col, tuketim_col, tarih_col, sozlesme_col, 
                  expected_year, selected_months):
    """Veri parÃ§asÄ±nÄ± iÅŸle"""
    try:
        # SÃ¼tunlarÄ± seÃ§
        df_chunk = chunk[[tn_col, tuketim_col, tarih_col, sozlesme_col]].copy()
        df_chunk.columns = ['TN', 'Tuketim', 'Tarih', 'Sozlesme_No']
        
        # BoÅŸ deÄŸerleri temizle
        df_chunk = df_chunk.dropna()
        
        if df_chunk.empty:
            return None
        
        # Tarihi Ã§evir
        df_chunk['Tarih'] = pd.to_datetime(df_chunk['Tarih'], errors='coerce')
        df_chunk = df_chunk.dropna(subset=['Tarih'])
        
        # YÄ±l filtresi
        df_chunk = df_chunk[df_chunk['Tarih'].dt.year == expected_year]
        
        # Ay filtresi
        if selected_months:
            df_chunk = df_chunk[df_chunk['Tarih'].dt.month.isin(selected_months)]
        
        if df_chunk.empty:
            return None
        
        # TÃ¼ketimi sayÄ±sal yap
        df_chunk['Tuketim'] = pd.to_numeric(df_chunk['Tuketim'], errors='coerce')
        df_chunk = df_chunk.dropna(subset=['Tuketim'])
        
        # Negatif deÄŸerleri temizle
        df_chunk = df_chunk[df_chunk['Tuketim'] >= 0]
        
        # GeÃ§miÅŸ veriler iÃ§in sÄ±fÄ±r tÃ¼ketim filtrele
        if expected_year in [2023, 2024]:
            df_chunk = df_chunk[df_chunk['Tuketim'] > 0]
        
        # String cleanup
        df_chunk['TN'] = df_chunk['TN'].astype(str).str.strip()
        df_chunk['Sozlesme_No'] = df_chunk['Sozlesme_No'].astype(str).str.strip()
        
        return df_chunk
        
    except Exception as e:
        st.warning(f"ParÃ§a iÅŸleme uyarÄ±sÄ±: {str(e)}")
        return None

def analyze_large_deviations(historical_avg, current_data, threshold):
    """BÃ¼yÃ¼k veri seti iÃ§in sapma analizi"""
    try:
        st.info("ğŸ” EÅŸleÅŸmeler bulunuyor...")
        
        # Memory efficient merge kullan
        merged_data = pd.merge(
            current_data, 
            historical_avg, 
            on=['TN', 'Sozlesme_No'], 
            how='inner'
        )
        
        if merged_data.empty:
            st.warning("âš ï¸ EÅŸleÅŸen tesisat bulunamadÄ±!")
            return pd.DataFrame()
        
        st.info(f"ğŸ“Š {len(merged_data)} adet eÅŸleÅŸme bulundu")
        
        # Sapma hesaplamalarÄ± (vectorized)
        merged_data['Sapma_MiktarÄ±'] = merged_data['Tuketim'] - merged_data['Ortalama_Tuketim']
        merged_data['Sapma_YÃ¼zdesi'] = (merged_data['Sapma_MiktarÄ±'] / merged_data['Ortalama_Tuketim']) * 100
        
        # SonuÃ§ DataFrame'i oluÅŸtur
        result_data = merged_data[[
            'TN', 'Sozlesme_No', 'Ay_Adi', 'Tarih',
            'Ortalama_Tuketim', 'Tuketim', 'Sapma_MiktarÄ±', 'Sapma_YÃ¼zdesi'
        ]].copy()
        
        result_data.columns = [
            'TN', 'Sozlesme_No', 'Ay', 'Tarih',
            'GeÃ§miÅŸ_Ortalama', 'GÃ¼ncel_Tuketim', 'Sapma_MiktarÄ±', 'Sapma_YÃ¼zdesi'
        ]
        
        # Bellek temizle
        del merged_data
        gc.collect()
        
        return result_data
        
    except Exception as e:
        st.error(f"Sapma analizi hatasÄ±: {str(e)}")
        return pd.DataFrame()

def display_large_results(deviation_results, threshold):
    """BÃ¼yÃ¼k veri sonuÃ§larÄ±nÄ± gÃ¶ster"""
    try:
        if deviation_results is None or deviation_results.empty:
            st.error("âŒ SonuÃ§ verisi bulunamadÄ±")
            return
        
        # Ã–zet metrikler
        total_compared = len(deviation_results)
        high_deviation = len(deviation_results[deviation_results['Sapma_YÃ¼zdesi'] >= threshold])
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Toplam KarÅŸÄ±laÅŸtÄ±rma", f"{total_compared:,}")
        with col2:
            st.metric(f">{threshold}% Sapma", f"{high_deviation:,}")
        with col3:
            ratio = (high_deviation/total_compared*100) if total_compared > 0 else 0
            st.metric("Sapma OranÄ±", f"{ratio:.1f}%")
        with col4:
            avg_deviation = deviation_results['Sapma_YÃ¼zdesi'].mean()
            st.metric("Ort. Sapma", f"{avg_deviation:.1f}%")
        
        # YÃ¼ksek sapma filter
        high_deviations = deviation_results[
            deviation_results['Sapma_YÃ¼zdesi'] >= threshold
        ].copy()
        
        if not high_deviations.empty:
            st.header(f"âš ï¸ {threshold}% Ãœzeri Sapma GÃ¶steren Tesisatlar")
            
            # SÄ±rala
            high_deviations = high_deviations.sort_values('Sapma_YÃ¼zdesi', ascending=False)
            
            # Ä°lk 1000 kaydÄ± gÃ¶ster (performance iÃ§in)
            display_count = min(1000, len(high_deviations))
            st.info(f"ğŸ“Š Ä°lk {display_count} kayÄ±t gÃ¶steriliyor (Toplam: {len(high_deviations)})")
            
            # Formatla ve gÃ¶ster
            display_df = format_display_table(high_deviations.head(display_count))
            st.dataframe(display_df, use_container_width=True, hide_index=True)
            
            # CSV indirme (Excel yerine daha hÄ±zlÄ±)
            csv_data = high_deviations.to_csv(index=False)
            st.download_button(
                label=f"ğŸ“¥ TÃ¼m Sapma Raporunu CSV Olarak Ä°ndir ({len(high_deviations)} kayÄ±t)",
                data=csv_data,
                file_name=f"sapma_raporu_{threshold}pct_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
                type="primary"
            )
            
            # Ä°statistikler
            st.subheader("ğŸ“Š Sapma Ä°statistikleri")
            stats_col1, stats_col2, stats_col3 = st.columns(3)
            
            with stats_col1:
                st.metric("En YÃ¼ksek Sapma", f"{high_deviations['Sapma_YÃ¼zdesi'].max():.1f}%")
            with stats_col2:
                st.metric("Ortalama Sapma", f"{high_deviations['Sapma_YÃ¼zdesi'].mean():.1f}%")
            with stats_col3:
                median_dev = high_deviations['Sapma_YÃ¼zdesi'].median()
                st.metric("Medyan Sapma", f"{median_dev:.1f}%")
        else:
            st.success(f"ğŸ‰ {threshold}% Ã¼zeri sapma gÃ¶steren tesisat bulunmamaktadÄ±r!")
            
    except Exception as e:
        st.error(f"SonuÃ§ gÃ¶sterimi hatasÄ±: {str(e)}")

def format_display_table(df):
    """GÃ¶rÃ¼ntÃ¼leme iÃ§in tabloyu formatla"""
    display_df = df.copy()
    
    # SayÄ±sal formatlar
    display_df['GeÃ§miÅŸ_Ortalama'] = display_df['GeÃ§miÅŸ_Ortalama'].apply(lambda x: f"{x:,.2f}")
    display_df['GÃ¼ncel_Tuketim'] = display_df['GÃ¼ncel_Tuketim'].apply(lambda x: f"{x:,.2f}")
    display_df['Sapma_MiktarÄ±'] = display_df['Sapma_MiktarÄ±'].apply(lambda x: f"{x:,.2f}")
    display_df['Sapma_YÃ¼zdesi'] = display_df['Sapma_YÃ¼zdesi'].apply(lambda x: f"{x:.1f}%")
    
    # SÃ¼tun baÅŸlÄ±klarÄ±
    display_df.columns = [
        'TN', 'SÃ¶zleÅŸme No', 'Ay', 'Tarih',
        'GeÃ§miÅŸ Ortalama', 'GÃ¼ncel TÃ¼ketim', 
        'Sapma MiktarÄ±', 'Sapma %'
    ]
    
    return display_df

if __name__ == "__main__":
    st.set_page_config(
        page_title="DoÄŸalgaz Sapma Analizi - BÃ¼yÃ¼k Veri",
        page_icon="âš¡",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Sayfa baÅŸÄ±nda bellek uyarÄ±sÄ±
    st.sidebar.markdown("---")
    st.sidebar.markdown("ğŸ’¾ **Bellek YÃ¶netimi**")
    st.sidebar.markdown("BÃ¼yÃ¼k dosyalar iÃ§in chunk boyutunu 25K-50K tutun")
    
    main()
