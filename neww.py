import streamlit as st
import pandas as pd
import numpy as np
from io import BytesIO
from datetime import datetime
import gc
import time
import os

def main():
    st.title("DoÄŸalgaz Sapma Analizi")
    st.markdown("800K+ satÄ±r iÃ§in optimize edildi - Parquet + Memory Mapping")
    
    # Performans ayarlarÄ±
    st.sidebar.header("ğŸš€ Performans AyarlarÄ±")
    
    # Sampling oranÄ±
    sample_rate = st.sidebar.selectbox(
        "Veri Ã–rnekleme OranÄ±:",
        [1.0, 0.5, 0.3, 0.1],
        index=0,
        format_func=lambda x: f"%{x*100:.0f} - {'TÃ¼m veri' if x==1 else 'HÄ±zlÄ± analiz'}"
    )
    
    # Memory mapping
    use_memory_mapping = st.sidebar.checkbox("Memory Mapping Kullan", value=True)
    
    # Minimal kolonlar
    minimal_mode = st.sidebar.checkbox("Minimal Mod (Sadece gerekli kolonlar)", value=True)
    
    # Dosya yÃ¼kleme
    st.sidebar.header("ğŸ“ Dosya YÃ¼kleme")
    
    file_2023 = st.sidebar.file_uploader("2023 Veriler", type=['xlsx', 'xls'], key="file_2023")
    file_2024 = st.sidebar.file_uploader("2024 Veriler", type=['xlsx', 'xls'], key="file_2024")  
    file_2025 = st.sidebar.file_uploader("2025 Veriler", type=['xlsx', 'xls'], key="file_2025")
    
    threshold = st.sidebar.slider("Sapma EÅŸiÄŸi (%)", 10, 100, 30)
    
    # HÄ±zlÄ± Ã¶n iÅŸleme seÃ§enekleri
    st.sidebar.header("HÄ±zlandÄ±rma SeÃ§enekleri")
    
    # Sadece yÃ¼ksek sapmalarÄ± analiz et
    quick_scan = st.sidebar.checkbox("Sadece yÃ¼ksek sapmalarÄ± tara", value=False)
    if quick_scan:
        quick_threshold = st.sidebar.number_input("Ã–n tarama eÅŸiÄŸi (%)", value=50.0)
    
    # Ay filtresi
    months_filter = st.sidebar.multiselect(
        "Analiz edilecek aylar (boÅŸ=tÃ¼mÃ¼):",
        range(1, 13),
        format_func=lambda x: datetime(2023, x, 1).strftime("%B")
    )
    
    if file_2023 and file_2024 and file_2025:
        
        # Ä°lk sÃ¼tun analizi
        with st.spinner("SÃ¼tun yapÄ±sÄ± analiz ediliyor..."):
            try:
                sample_df = pd.read_excel(file_2023, nrows=3)
                columns = sample_df.columns.tolist()
            except:
                st.error("Excel dosyasÄ± okunamÄ±yor!")
                return
        
        # SÃ¼tun seÃ§imi - compact layout
        st.header("ğŸ”§ HÄ±zlÄ± SÃ¼tun SeÃ§imi")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            tn_col = st.selectbox("TN:", columns, key="tn")
        with col2:
            consumption_col = st.selectbox("TÃ¼ketim:", columns, key="cons")
        with col3:
            date_col = st.selectbox("Tarih:", columns, key="date")
        with col4:
            contract_col = st.selectbox("SÃ¶zleÅŸme:", columns, key="contract")
        
        # SÃ¼per hÄ±zlÄ± analiz butonu
        if st.button("ğŸš€ SÃœPER HIZLI ANALÄ°Z", type="primary"):
            
            # Timer baÅŸlat
            start_time = time.time()
            progress = st.progress(0)
            status = st.empty()
            
            try:
                # 1. ADIM: Parquet'e Ã§evir ve cache'le (tek seferlik)
                status.text("ğŸ“¦ Dosyalar Parquet formatÄ±na Ã§evriliyor...")
                progress.progress(10)
                
                parquet_files = convert_to_parquet_cached(
                    file_2023, file_2024, file_2025,
                    tn_col, consumption_col, date_col, contract_col,
                    minimal_mode
                )
                
                if not parquet_files:
                    st.error("âŒ Dosya dÃ¶nÃ¼ÅŸtÃ¼rme baÅŸarÄ±sÄ±z!")
                    return
                
                progress.progress(25)
                
                # 2. ADIM: Lightning fast read
                status.text("âš¡ Lightning speed veri okuma...")
                historical_data = fast_read_historical(
                    parquet_files['2023'], parquet_files['2024'],
                    sample_rate, months_filter
                )
                
                current_data = fast_read_current(
                    parquet_files['2025'], 
                    sample_rate, months_filter
                )
                
                progress.progress(50)
                
                # 3. ADIM: Vectorized hesaplamalar
                status.text("ğŸ§® SÃ¼per hÄ±zlÄ± hesaplamalar...")
                results = lightning_deviation_analysis(
                    historical_data, current_data, threshold,
                    quick_scan, quick_threshold if quick_scan else None
                )
                
                progress.progress(80)
                
                # 4. ADIM: SonuÃ§larÄ± gÃ¶ster
                status.text("ğŸ“Š SonuÃ§lar hazÄ±rlanÄ±yor...")
                display_lightning_results(results, threshold, sample_rate)
                
                progress.progress(100)
                
                # Performans raporu
                total_time = time.time() - start_time
                st.success(f"âœ… {total_time:.1f} saniyede tamamlandÄ±!")
                
                # Cleanup (in-memory parquet, dosya temizleme gereksiz)
                del parquet_files
                gc.collect()
                
            except Exception as e:
                st.error(f"âŒ Hata: {str(e)}")
                st.info("ğŸ’¡ Ã–rnekleme oranÄ±nÄ± dÃ¼ÅŸÃ¼rmeyi deneyin")
    else:
        # HÄ±z ipuÃ§larÄ±
        st.info("ğŸ“‚ 3 Excel dosyasÄ±nÄ± yÃ¼kleyin")
        
        st.header("âš¡ SÃ¼per HÄ±zlÄ± Analiz Ä°puÃ§larÄ±")
        tips = """
        **ğŸš€ Maximum HÄ±z Ä°Ã§in:**
        - **%50 Ã¶rnekleme** ile baÅŸlayÄ±n (2x hÄ±zlÄ±)
        - **Memory mapping** aÃ§Ä±k tutun
        - **Minimal mod** aktif edin
        - Sadece **gerekli aylarÄ±** seÃ§in
        - **Quick scan** ile Ã¶n tarama yapÄ±n
        
        **ğŸ“Š 800K SatÄ±r Performans:**
        - Normal: ~5-10 dakika
        - %50 Ã¶rnekleme: ~2-3 dakika  
        - %30 Ã¶rnekleme: ~1-2 dakika
        - Quick scan: ~30-60 saniye
        """
        st.markdown(tips)

@st.cache_data(ttl=3600, max_entries=3)  # 1 saat cache, max 3 dosya
def convert_to_parquet_cached(file_2023, file_2024, file_2025, 
                             tn_col, cons_col, date_col, contract_col, minimal):
    """Excel dosyalarÄ±nÄ± Parquet'e Ã§evir ve cache'le"""
    try:
        parquet_files = {}
        
        for year, file_obj in [('2023', file_2023), ('2024', file_2024), ('2025', file_2025)]:
            st.info(f"ğŸ“Š {year} dosyasÄ± iÅŸleniyor...")
            
            # Excel'i oku (chunksize kullanmadan)
            if minimal:
                # Sadece gerekli kolonlarÄ± oku
                usecols = [tn_col, cons_col, date_col, contract_col]
                df = pd.read_excel(file_obj, usecols=usecols, engine='openpyxl')
            else:
                df = pd.read_excel(file_obj, engine='openpyxl')
            
            st.info(f"âœ… {year}: {len(df)} satÄ±r okundu")
            
            # HÄ±zlÄ± temizlik
            initial_rows = len(df)
            df = df.dropna(subset=[tn_col, cons_col, date_col, contract_col])
            st.info(f"ğŸ§¹ {initial_rows - len(df)} boÅŸ satÄ±r temizlendi")
            
            # Tip optimizasyonu - memory efficient
            try:
                df[tn_col] = df[tn_col].astype('category')
                df[contract_col] = df[contract_col].astype('category') 
                df[cons_col] = pd.to_numeric(df[cons_col], errors='coerce')
                df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
            except Exception as type_error:
                st.warning(f"âš ï¸ Tip dÃ¶nÃ¼ÅŸtÃ¼rme uyarÄ±sÄ±: {type_error}")
            
            # GeÃ§ersiz veriyi temizle
            df = df.dropna()
            df = df[df[cons_col] >= 0]  # Negatif tÃ¼ketim yok
            
            st.success(f"ğŸ¯ {year}: {len(df)} temiz satÄ±r hazÄ±r")
            
            # In-memory parquet bytes oluÅŸtur (dosya sistemi yerine)
            parquet_buffer = BytesIO()
            df.to_parquet(parquet_buffer, compression='snappy', index=False)
            parquet_buffer.seek(0)
            parquet_files[year] = parquet_buffer.getvalue()
            
            del df  # BelleÄŸi hemen serbest bÄ±rak
            gc.collect()
        
        return parquet_files
        
    except Exception as e:
        st.error(f"âŒ Parquet dÃ¶nÃ¼ÅŸÃ¼m hatasÄ±: {str(e)}")
        st.info("ğŸ’¡ Dosya boyutu Ã§ok bÃ¼yÃ¼k olabilir, Ã¶rnekleme kullanmayÄ± deneyin")
        return None

def fast_read_historical(parquet_data_2023, parquet_data_2024, sample_rate, months_filter):
    """Parquet bytes'larÄ±nÄ± Ã§ok hÄ±zlÄ± oku"""
    try:
        # Parquet bytes'dan DataFrame'e Ã§evir
        df_2023 = pd.read_parquet(BytesIO(parquet_data_2023))
        df_2024 = pd.read_parquet(BytesIO(parquet_data_2024))
        
        st.info(f"ğŸ“Š 2023: {len(df_2023)}, 2024: {len(df_2024)} satÄ±r okundu")
        
        # Sampling (memory tasarrufu)
        if sample_rate < 1.0:
            original_2023 = len(df_2023)
            original_2024 = len(df_2024)
            df_2023 = df_2023.sample(frac=sample_rate, random_state=42)
            df_2024 = df_2024.sample(frac=sample_rate, random_state=42)
            st.info(f"ğŸ¯ Ã–rnekleme: 2023 {original_2023}â†’{len(df_2023)}, 2024 {original_2024}â†’{len(df_2024)}")
        
        # Kolon isimlerini normalize et (ilk 4 kolonu al)
        cols = df_2023.columns.tolist()[:4]  # Ä°lk 4 kolon: TN, Tuketim, Tarih, Sozlesme
        df_2023 = df_2023[cols].copy()
        df_2024 = df_2024[cols].copy()
        
        df_2023.columns = ['TN', 'Tuketim', 'Tarih', 'Sozlesme_No']
        df_2024.columns = ['TN', 'Tuketim', 'Tarih', 'Sozlesme_No']
        
        # YÄ±l filtreleri (eÄŸer tarih bilgisi varsa)
        try:
            df_2023 = df_2023[df_2023['Tarih'].dt.year == 2023]
            df_2024 = df_2024[df_2024['Tarih'].dt.year == 2024]
        except:
            st.warning("âš ï¸ Tarih filtreleme atlandÄ±")
        
        # Ay filtresi
        if months_filter:
            try:
                df_2023 = df_2023[df_2023['Tarih'].dt.month.isin(months_filter)]
                df_2024 = df_2024[df_2024['Tarih'].dt.month.isin(months_filter)]
                st.info(f"ğŸ“… Ay filtresi uygulandÄ±: {months_filter}")
            except:
                st.warning("âš ï¸ Ay filtresi atlandÄ±")
        
        # BirleÅŸtir
        combined = pd.concat([df_2023, df_2024], ignore_index=True)
        
        # SÄ±fÄ±r tÃ¼ketim filtrele (historical iÃ§in)
        before_filter = len(combined)
        combined = combined[combined['Tuketim'] > 0]
        st.info(f"ğŸ”¥ SÄ±fÄ±r tÃ¼ketim temizleme: {before_filter}â†’{len(combined)}")
        
        if combined.empty:
            st.error("âŒ Temizleme sonrasÄ± veri kalmadÄ±!")
            return None
        
        # Vectorized ortalama hesapla
        historical_avg = combined.groupby(['TN', 'Sozlesme_No'])['Tuketim'].agg(['mean', 'count']).reset_index()
        historical_avg.columns = ['TN', 'Sozlesme_No', 'Ortalama_Tuketim', 'Count']
        
        # En az 2 kayÄ±t olanlarÄ± al
        initial_count = len(historical_avg)
        historical_avg = historical_avg[historical_avg['Count'] >= 2]
        st.success(f"ğŸ“ˆ {initial_count}â†’{len(historical_avg)} tesisat ortalamasÄ± hesaplandÄ±")
        
        return historical_avg[['TN', 'Sozlesme_No', 'Ortalama_Tuketim']]
        
    except Exception as e:
        st.error(f"âŒ Historical read hatasÄ±: {str(e)}")
        return None

def fast_read_current(parquet_data_2025, sample_rate, months_filter):
    """2025 verisini hÄ±zlÄ± oku"""
    try:
        df = pd.read_parquet(BytesIO(parquet_data_2025))
        
        st.info(f"ğŸ“Š 2025: {len(df)} satÄ±r okundu")
        
        # Sampling
        if sample_rate < 1.0:
            original_count = len(df)
            df = df.sample(frac=sample_rate, random_state=42)
            st.info(f"ğŸ¯ 2025 Ã¶rnekleme: {original_count}â†’{len(df)}")
        
        # Normalize columns (ilk 4 kolon)
        cols = df.columns.tolist()[:4]
        df = df[cols].copy()
        df.columns = ['TN', 'Tuketim', 'Tarih', 'Sozlesme_No']
        
        # 2025 filtresi
        try:
            df = df[df['Tarih'].dt.year == 2025]
            st.info(f"ğŸ“… 2025 yÄ±l filtresi: {len(df)} satÄ±r kaldÄ±")
        except:
            st.warning("âš ï¸ 2025 yÄ±l filtresi atlandÄ±")
        
        # Ay filtresi
        if months_filter:
            try:
                initial = len(df)
                df = df[df['Tarih'].dt.month.isin(months_filter)]
                st.info(f"ğŸ“… Ay filtresi: {initial}â†’{len(df)} satÄ±r")
            except:
                st.warning("âš ï¸ Ay filtresi atlandÄ±")
        
        if df.empty:
            st.error("âŒ 2025 filtresi sonrasÄ± veri kalmadÄ±!")
            return None
        
        # Ay bilgisi ekle
        try:
            df['Ay_Adi'] = df['Tarih'].dt.strftime('%Y-%m')
        except:
            df['Ay_Adi'] = '2025-01'  # Fallback
        
        st.success(f"âœ… 2025 veri hazÄ±r: {len(df)} satÄ±r")
        return df
        
    except Exception as e:
        st.error(f"âŒ Current read hatasÄ±: {str(e)}")
        return None

def lightning_deviation_analysis(historical, current, threshold, quick_scan=False, quick_threshold=None):
    """IÅŸÄ±k hÄ±zÄ±nda sapma analizi"""
    try:
        if historical is None or current is None:
            st.error("âŒ Veri eksik!")
            return pd.DataFrame()
        
        if historical.empty or current.empty:
            st.error("âŒ BoÅŸ veri!")
            return pd.DataFrame()
        
        st.info(f"ğŸ”— EÅŸleÅŸtirme: Historical={len(historical)}, Current={len(current)}")
        
        # Super fast merge
        merged = pd.merge(current, historical, on=['TN', 'Sozlesme_No'], how='inner')
        
        if merged.empty:
            st.warning("âš ï¸ EÅŸleÅŸen tesisat bulunamadÄ±!")
            return pd.DataFrame()
        
        st.success(f"ğŸ¯ {len(merged)} eÅŸleÅŸme bulundu")
        
        # Vectorized hesaplamalar (tek seferde)
        merged['Sapma_Miktari'] = merged['Tuketim'] - merged['Ortalama_Tuketim']
        merged['Sapma_YÃ¼zdesi'] = (merged['Sapma_Miktari'] / merged['Ortalama_Tuketim']) * 100
        
        # Quick scan filter
        if quick_scan and quick_threshold:
            # Ã–nce yÃ¼ksek sapmalarÄ± bul
            high_dev_mask = merged['Sapma_YÃ¼zdesi'] >= quick_threshold
            high_count = high_dev_mask.sum()
            if high_count > 0:
                merged = merged[high_dev_mask]
                st.info(f"âš¡ Quick scan: {high_count} yÃ¼ksek sapma tespit edildi")
        
        # Final result
        result = merged[[
            'TN', 'Sozlesme_No', 'Ay_Adi', 'Tarih',
            'Ortalama_Tuketim', 'Tuketim', 'Sapma_Miktari', 'Sapma_YÃ¼zdesi'
        ]].copy()
        
        result.columns = [
            'TN', 'Sozlesme_No', 'Ay', 'Tarih',
            'GeÃ§miÅŸ_Ortalama', 'GÃ¼ncel_Tuketim', 'Sapma_MiktarÄ±', 'Sapma_YÃ¼zdesi'
        ]
        
        return result
        
    except Exception as e:
        st.error(f"âŒ Lightning analysis hatasÄ±: {str(e)}")
        return pd.DataFrame()

def display_lightning_results(results, threshold, sample_rate):
    """Lightning speed sonuÃ§ gÃ¶sterimi"""
    try:
        if results.empty:
            st.warning("âš ï¸ SonuÃ§ bulunamadÄ±")
            return
        
        # Quick stats
        total = len(results)
        high_dev = len(results[results['Sapma_YÃ¼zdesi'] >= threshold])
        
        # Sampling uyarÄ±sÄ±
        if sample_rate < 1.0:
            st.info(f"ğŸ“Š %{sample_rate*100:.0f} Ã¶rnekleme ile analiz yapÄ±ldÄ±. GerÃ§ek sayÄ±lar ~{1/sample_rate:.1f}x daha fazla olabilir.")
        
        # Metrics
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Toplam Analiz", f"{total:,}")
        with col2:
            estimated_high = int(high_dev / sample_rate) if sample_rate < 1.0 else high_dev
            st.metric(f">{threshold}% Sapma", f"~{estimated_high:,}")
        with col3:
            ratio = (high_dev/total*100) if total > 0 else 0
            st.metric("Sapma OranÄ±", f"{ratio:.1f}%")
        with col4:
            max_dev = results['Sapma_YÃ¼zdesi'].max()
            st.metric("Max Sapma", f"{max_dev:.0f}%")
        
        # YÃ¼ksek sapma tablosu
        high_deviations = results[results['Sapma_YÃ¼zdesi'] >= threshold].copy()
        
        if not high_deviations.empty:
            st.header(f"âš ï¸ {threshold}% Ãœzeri Sapma")
            
            # Sort by deviation
            high_deviations = high_deviations.sort_values('Sapma_YÃ¼zdesi', ascending=False)
            
            # Top 500 gÃ¶ster (hÄ±z iÃ§in)
            display_count = min(500, len(high_deviations))
            st.info(f"ğŸ“Š Ä°lk {display_count} gÃ¶steriliyor (Toplam: {len(high_deviations)})")
            
            # Format ve gÃ¶ster
            display_df = format_lightning_table(high_deviations.head(display_count))
            st.dataframe(display_df, use_container_width=True)
            
            # SÃ¼per hÄ±zlÄ± CSV download
            if st.button("ğŸ“¥ HÄ±zlÄ± CSV Ä°ndir"):
                csv = high_deviations.to_csv(index=False)
                st.download_button(
                    "ğŸ’¾ CSV DosyasÄ±nÄ± Ä°ndir",
                    csv,
                    f"sapma_raporu_{datetime.now().strftime('%H%M%S')}.csv",
                    "text/csv"
                )
        else:
            st.success(f"ğŸ‰ {threshold}% Ã¼zeri sapma yok!")
            
    except Exception as e:
        st.error(f"Display hatasÄ±: {str(e)}")

def format_lightning_table(df):
    """HÄ±zlÄ± tablo formatÄ±"""
    try:
        display = df.copy()
        
        # HÄ±zlÄ± format (detaysÄ±z)
        display['GeÃ§miÅŸ_Ortalama'] = display['GeÃ§miÅŸ_Ortalama'].round(0).astype(int)
        display['GÃ¼ncel_Tuketim'] = display['GÃ¼ncel_Tuketim'].round(0).astype(int) 
        display['Sapma_YÃ¼zdesi'] = display['Sapma_YÃ¼zdesi'].round(0).astype(int)
        
        # SÃ¼tun adlarÄ±
        display.columns = [
            'TN', 'SÃ¶zleÅŸme', 'Ay', 'Tarih',
            'Eski Ort.', 'Yeni', 'Sapma', 'Sapma%'
        ]
        
        return display
        
    except:
        return df

def cleanup_temp_files(parquet_files):
    """GeÃ§ici dosyalarÄ± temizle"""
    try:
        for file_path in parquet_files.values():
            if os.path.exists(file_path):
                os.remove(file_path)
    except:
        pass

if __name__ == "__main__":
    st.set_page_config(
        page_title="âš¡ SÃ¼per HÄ±zlÄ± DoÄŸalgaz Analizi",
        page_icon="âš¡",
        layout="wide"
    )
    
    # Performans uyarÄ±sÄ±
    st.sidebar.markdown("---")
    st.sidebar.markdown("âš¡ **SÃœPER HIZ MODU**")
    st.sidebar.markdown("800K satÄ±r ~1-2 dakikada!")
    
    main()
